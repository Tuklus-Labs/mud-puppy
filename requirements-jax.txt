# Mud-Puppy JAX/Flax Requirements (Fine-Tuning Framework)
# Generated: 2026-01-26
#
# Installation:
#   TPU:  pip install -r requirements-jax.txt --extra-index-url https://storage.googleapis.com/jax-releases/libtpu_releases.html
#   CUDA: pip install -r requirements-jax.txt
#   CPU:  pip install -r requirements-jax.txt
#
# For Qwix quantization (QLoRA, not on PyPI):
#   pip install git+https://github.com/google/qwix.git

# ==============================================================================
# CORE JAX ECOSYSTEM
# ==============================================================================
jax==0.9.0
jaxlib==0.9.0
flax==0.12.2
optax==0.2.6
orbax-checkpoint==0.11.32
chex==0.1.88

# ==============================================================================
# TPU-SPECIFIC (uncomment for TPU deployment)
# ==============================================================================
# libtpu-nightly  # Required for Cloud TPU
# To install JAX with TPU support:
# pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html

# ==============================================================================
# CUDA-SPECIFIC (uncomment for NVIDIA GPU)
# ==============================================================================
# jax-cuda12-plugin==0.9.0
# To install JAX with CUDA support:
# pip install jax[cuda12]

# ==============================================================================
# ROCM-SPECIFIC (uncomment for AMD GPU)
# ==============================================================================
# jax-rocm60-plugin==0.9.0
# To install JAX with ROCm support:
# pip install jax[rocm]

# ==============================================================================
# DATA LOADING & DATASETS
# ==============================================================================
grain==0.2.12
datasets==4.5.0
tensorflow-datasets==4.9.6
arrayrecord==0.5.1

# ==============================================================================
# HUGGINGFACE ECOSYSTEM
# ==============================================================================
transformers==4.48.1
tokenizers==0.21.0
huggingface-hub==0.28.1
safetensors==0.5.2
accelerate==1.3.0

# ==============================================================================
# NUMERICAL / SCIENTIFIC
# ==============================================================================
numpy==1.26.4
scipy==1.14.1
ml-dtypes==0.5.1

# ==============================================================================
# OPTAX SCHEDULERS & ADVANCED OPTIMIZERS
# ==============================================================================
# optax already provides these schedulers out of the box:
# - warmup_cosine_decay_schedule
# - warmup_exponential_decay_schedule
# - linear_schedule
# - piecewise_constant_schedule
# - polynomial_schedule
# - cosine_decay_schedule
# - exponential_decay
# - sgdr_schedule (warm restarts)
#
# For Shampoo optimizer (second-order):
# optax-shampoo==0.0.6

# ==============================================================================
# EXPERIMENT TRACKING
# ==============================================================================
wandb==0.19.3
tensorboard==2.18.0

# ==============================================================================
# UTILITIES
# ==============================================================================
tqdm==4.67.1
rich==13.9.4
pyyaml==6.0.2
einops==0.8.0
tensorstore==0.1.75

# ==============================================================================
# SERIALIZATION / CHECKPOINTING
# ==============================================================================
msgpack==1.1.0
protobuf==5.29.3

# ==============================================================================
# ASYNC / NETWORKING (for distributed training)
# ==============================================================================
nest-asyncio==1.6.0
aiofiles==24.1.0

# ==============================================================================
# PROFILING (optional)
# ==============================================================================
# xprof==0.9.0
# tensorboard-plugin-profile==2.18.0

# ==============================================================================
# DEVELOPMENT (optional)
# ==============================================================================
# pytest==8.3.4
# pytest-xdist==3.6.1
# black==24.10.0
# ruff==0.8.6
# mypy==1.13.0

# ==============================================================================
# FINE-TUNING CONFIGURATION NOTES
# ==============================================================================
#
# LoRA Configuration (via Flax NNX):
#   - rank: 8-64 typical
#   - alpha: usually 2x rank
#   - dropout: 0.05-0.1
#   - target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
#
# QLoRA Configuration (with Qwix):
#   - base_model: quantized to INT4/NF4
#   - lora_adapters: FP16/BF16
#   - Use qwix.QuantizationRule for layer-specific quantization
#
# Recommended Learning Rates:
#   - Full fine-tuning: 1e-5 to 5e-5
#   - LoRA: 1e-4 to 3e-4
#   - QLoRA: 2e-4 to 5e-4
#
# Batch Size Guidelines (TPU v4-8):
#   - 7B model: 32-64 per chip
#   - 13B model: 16-32 per chip
#   - 70B model: 4-8 per chip (with model parallelism)
